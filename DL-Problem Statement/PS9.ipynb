{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOcj/xvAvd+qJvccR9QmVq3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EABluFVzf35g","executionInfo":{"status":"ok","timestamp":1697946813329,"user_tz":-330,"elapsed":6,"user":{"displayName":"PRITISH RAJPUROHIT","userId":"02325941640202183678"}},"outputId":"e178b4b4-e241-40f0-d5b8-1aa95f4724dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Time Step 1 - Hidden State: [-0.96173206 -0.58496839 -0.96886184 -0.92659414] - Output: [-3.88273013  0.78470417]\n","Time Step 2 - Hidden State: [ 0.86873635  0.99377747  0.99890617 -0.99014082] - Output: [ 1.5235875  -4.66207969]\n","Time Step 3 - Hidden State: [ 0.80532323  0.9597728  -0.66760942  0.17261006] - Output: [ 1.55791062 -3.77683017]\n","Time Step 4 - Hidden State: [ 0.76548557 -0.83975859 -0.99327807  0.36350352] - Output: [-1.12558172 -0.03380735]\n","Time Step 5 - Hidden State: [-0.53632566 -0.24763753 -0.99994757  0.97481815] - Output: [-2.02078978  0.0469496 ]\n"]}],"source":["import numpy as np\n","\n","# Hyperparameters\n","input_size = 3  # Size of the input vector\n","hidden_size = 4  # Size of the hidden state\n","output_size = 2  # Size of the output\n","sequence_length = 5  # Length of the input sequence\n","\n","# Random seed for reproducibility\n","np.random.seed(0)\n","\n","# Initialize weights and biases\n","Wx = np.random.randn(hidden_size, input_size)  # Input-to-hidden weights\n","Wh = np.random.randn(hidden_size, hidden_size)  # Hidden-to-hidden weights\n","Wy = np.random.randn(output_size, hidden_size)  # Hidden-to-output weights\n","bx = np.random.randn(hidden_size, 1)  # Input-to-hidden bias\n","by = np.random.randn(output_size, 1)  # Hidden-to-output bias\n","\n","# Initial hidden state\n","h = np.zeros((hidden_size, 1))\n","\n","# Input data (a random sequence of input vectors)\n","X = np.random.randn(input_size, sequence_length)\n","\n","# List to store the hidden states over time\n","hidden_states = []\n","\n","# List to store the outputs over time\n","outputs = []\n","\n","for t in range(sequence_length):\n","    x_t = X[:, t:t+1]  # Input at time step t\n","    a = np.dot(Wx, x_t) + np.dot(Wh, h) + bx  # Weighted sum of inputs and previous hidden state\n","    h = np.tanh(a)  # Hidden state (using hyperbolic tangent activation)\n","    y = np.dot(Wy, h) + by  # Output\n","    hidden_states.append(h)\n","    outputs.append(y)\n","\n","# Print the hidden states and outputs\n","for t in range(sequence_length):\n","    print(f\"Time Step {t + 1} - Hidden State: {hidden_states[t].flatten()} - Output: {outputs[t].flatten()}\")\n"]},{"cell_type":"code","source":["# Generate random labels for the example\n","# In a real task, you would have actual labels for your data\n","true_labels = np.random.randint(0, output_size, size=sequence_length)\n","\n","# Simulate predicted labels (e.g., by taking the argmax of the RNN's output)\n","predicted_labels = [np.argmax(output) for output in outputs]\n","\n","# Calculate accuracy\n","correct_predictions = sum(1 for true, predicted in zip(true_labels, predicted_labels) if true == predicted)\n","accuracy = correct_predictions / sequence_length\n","\n","print(f\"True Labels: {true_labels}\")\n","print(f\"Predicted Labels: {predicted_labels}\")\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qrmBJ8GdhX5z","executionInfo":{"status":"ok","timestamp":1697946899530,"user_tz":-330,"elapsed":405,"user":{"displayName":"PRITISH RAJPUROHIT","userId":"02325941640202183678"}},"outputId":"43bcea68-39c8-4ceb-e050-a0a8ca3b6745"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["True Labels: [1 1 0 0 0]\n","Predicted Labels: [1, 0, 0, 1, 1]\n","Accuracy: 40.00%\n"]}]}]}